# Terraform remote state setup.
RESOURCE_GROUP_NAME=tfstate
BLOB_CONTAINER=tfstate
BLOB_KEY_NAME=terraform.tfstate

# The Event Hub aka. Kafka topic we will publish to.
TOPIC="order"
PARTITION_COUNT="3"

# The log level to use across all the applications.
LOG_LEVEL="INFO"

# Snowflake Config
SNOWFLAKE_DATABASE="MY_ONLINE_SHOP"
SNOWFLAKE_SCHEMA="ONLINE_SHOP"
# Role, warehouse and file format identifier for bulk copying.
BULK_COPY_WAREHOUSE="BULK_COPY_WAREHOUSE"
BULK_LOAD_ROLE="COPY_CUSTOMER_ROLE"
CUSTOMER_JSON_FORMAT="customer_json_format"
# Role and warehouse to loading stream data.
LOAD_STREAM_WAREHOUSE="LOAD_STREAM_WAREHOUSE"
STREAMING_DATA_ROLE="LOAD_ORDER_STREAM_ROLE"

# Config for creating synthetic customer data in the bulk loader.
CUSTOMER_TABLE="CUSTOMER"
NUMBER_OF_CUSTOMERS=10

# Config for producing synthetic order data.
PRODUCER_COUNT=4
# aka. number of orders to generate.
PRODUCED_MESSAGES_COUNT=50
# Number of request to concurrently handle in the producer.
PRODUCED_BATCH_SIZE=15
# Total number of messages to consume and log in the consumer
CONSUMED_TOTAL_MESSAGES=30
# Number of request to concurrently handle in the consumer.
CONSUMED_BATCH_SIZE=5 

# How to run the kafka connector in local development, one of 'produce', 'consume' or 'both'.
MODE='both' 